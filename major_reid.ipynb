{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "major_reid.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDooQ21buIox",
        "colab_type": "text"
      },
      "source": [
        "<h1>Person re-identification Project</h1>\n",
        "<p>\n",
        "<h4>This notebook contains the code done as part of the major project at Pulchowk Campus.</h4>\n",
        "Notebook contains code to train and test the model using Market1501 dataset.</p>\n",
        "\n",
        "<i>Link to the kaggle dataset  \n",
        "https://www.kaggle.com/rayiooo/reid_market-1501\n",
        "\n",
        "**Contributor**: *Krishna Prasad Panthi*</i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSaRZDEYibb1",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a68b62dc-3003-4ee9-95f0-2f8e66fbdc2a"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0b357669-c28f-40c1-800b-7e582f3145b7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0b357669-c28f-40c1-800b-7e582f3145b7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHNrfYXotjSW",
        "colab_type": "text"
      },
      "source": [
        "**Downloading data from kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpAQ9S4nixB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "#change permission\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKzhGsGri3Tg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "df849175-b82f-4e37-e158-3f7f19cea97c"
      },
      "source": [
        "!kaggle datasets download -d pengcw1/market-1501"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading market-1501.zip to /content\n",
            " 95% 138M/146M [00:01<00:00, 85.6MB/s]\n",
            "100% 146M/146M [00:01<00:00, 84.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVROOfLuw3Ia",
        "colab_type": "text"
      },
      "source": [
        "**Extracting data to the dataset folder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi2qBB3ji7ND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip market-1501.zip\n",
        "!mv Market-1501-v15.09.15 dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx9jJV-fxBaR",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Import libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg018hXoi_38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pickle\n",
        "import os\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpVtXvl4xhJn",
        "colab_type": "text"
      },
      "source": [
        "<h1>Load training data</h1>\n",
        "Saves the training data in <i>\"train_dataset.pkl\"</i> file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOe0xAbHxGw_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12cbcd74-087a-4a9b-f3fc-b5ac792bbc35"
      },
      "source": [
        "def load_trains(dataset_path):\n",
        "    '''\n",
        "    arguments:\n",
        "    dataset_path : system path to the folder containing the datasets\n",
        "                   assumes bounding_box_train contains the training images\n",
        "                   and bounding_box_test contains the testing images\n",
        "\n",
        "                   name of image file is assumed to be with imagelabels_whatever.jpg\n",
        "\n",
        "    saves to train_dataset.pkl:\n",
        "    X_train: 3D numpy array of all training images with images\n",
        "\n",
        "    Y_train : 1D numpy array of labels corresponding to each label list.\n",
        "              i.e. labels are not repeated.\n",
        "    '''\n",
        "\n",
        "    #get the system paths for the training image folder\n",
        "    train_images_path = dataset_path + '/bounding_box_train'\n",
        "    # train_images_path = dataset_path + '/gt_bbox'\n",
        "\n",
        "    #sort the image files in order of labels\n",
        "    train_images_list = sorted(os.listdir(train_images_path))\n",
        "\n",
        "    X_train = []\n",
        "\n",
        "    train_labels = []\n",
        "\n",
        "\n",
        "    #loop through the image list\n",
        "    for i in train_images_list:\n",
        "        #check if file name ends with .jpg\n",
        "        if i.split('.')[-1] == 'jpg':\n",
        "            #read the image if true\n",
        "            img = cv2.imread(os.path.join(train_images_path,i))\n",
        "            X_train.append(img)\n",
        "            #read the label from the file name\n",
        "            train_labels.append(int(i.split('_')[0]))\n",
        "\n",
        "\n",
        "    #convert all python lists to numpy arrays\n",
        "    X_train = np.array(X_train)\n",
        "    train_labels = np.array(train_labels)\n",
        "    \n",
        "    #split the images with same labels into different lists\n",
        "    unique,indices,inverses,counts = np.unique(train_labels,return_index=True,return_inverse=True,return_counts=True)\n",
        "    X_train = np.split(X_train,indices)\n",
        "    del X_train[0]\n",
        "    X_train = np.array(X_train)\n",
        "    Y_train = np.unique(inverses)\n",
        "\n",
        "\n",
        "\n",
        "    return X_train,Y_train\n",
        "\n",
        "X_train,Y_train= load_trains('dataset')\n",
        "file = open('train_dataset.pkl','wb')\n",
        "pickle.dump([X_train,Y_train],file)\n",
        "file.close()\n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(751,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgUGc-Jvy3ke",
        "colab_type": "text"
      },
      "source": [
        "<h1>Load testing data</h1>\n",
        "Saves the training data in <i>\"test_dataset.pkl\"</i> file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImobJzlsjJKy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "663eb406-c1f0-4f33-e631-8417f55461c7"
      },
      "source": [
        "def load_tests(dataset_path):\n",
        "    '''\n",
        "    arguments:\n",
        "    dataset_path : system path to the folder containing the datasets\n",
        "                   assumes bounding_box_test contains the testing images\n",
        "                   and bounding_box_test contains the testing images\n",
        "\n",
        "                   name of image file is assumed to be with imagelabels_whatever.jpg\n",
        "\n",
        "    Returns:\n",
        "    saves data to the pickle file test_dataset.pkl \n",
        "    test_images, test_images_names, query_images, query_images_name\n",
        "    '''\n",
        "\n",
        "    #get the system paths for the testing image folder\n",
        "    test_images_path = dataset_path + '/bounding_box_test'\n",
        "\n",
        "    #get the system path fot the testing image folder\n",
        "    query_images_path = dataset_path + '/query'\n",
        "\n",
        "    #sort the image files in order of labels\n",
        "    test_images_list = os.listdir(test_images_path)\n",
        "    query_images_list = os.listdir(query_images_path)\n",
        "    del test_images_list[test_images_list.index('Thumbs.db')]\n",
        "    del query_images_list[query_images_list.index('Thumbs.db')]\n",
        "\n",
        "\n",
        "    test_img  = []\n",
        "    query_img = []\n",
        "\n",
        "\n",
        "    #loop through the image list\n",
        "    for i in test_images_list:\n",
        "        #check if file name ends with .jpg\n",
        "        if i.split('.')[-1] == 'jpg':\n",
        "            #read the image if true\n",
        "            img = cv2.imread(os.path.join(test_images_path,i))\n",
        "            test_img.append(img)\n",
        "            \n",
        "\n",
        "\n",
        "    for i in query_images_list:\n",
        "        #check if file name end with .jpg and ignore files with labels 0 or -1\n",
        "        if i.split('.')[-1] == 'jpg' and int(i.split('_')[0])!=-1 and int(i.split('_')[0])!=0:\n",
        "            img = cv2.imread(os.path.join(query_images_path,i))\n",
        "            query_img.append(img)\n",
        "\n",
        "    #convert all python lists to numpy arrays\n",
        "    test_img = np.array(test_img)\n",
        "    query_img = np.array(query_img)\n",
        "    test_img_names = np.array(test_images_list)\n",
        "    query_img_names = np.array(query_images_list)\n",
        "\n",
        "\n",
        "    return test_img,test_img_names,query_img,query_img_names\n",
        "\n",
        "\n",
        "t,t_n,q,q_n = load_tests('dataset')\n",
        "file = open('test_dataset.pkl','wb')\n",
        "pickle.dump([t,t_n,q,q_n],file)\n",
        "file.close()\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYXjSSfry9M-",
        "colab_type": "text"
      },
      "source": [
        "<h1>Loss Functions</h1>\n",
        "<h4>Here we use combinations of three loss functions</h4>\n",
        "<ul>\n",
        "<li>Categorical cross entropy loss</li>\n",
        "<li>Triplet loss</li>\n",
        "<li>Weighted Contrastive loss <a href=https://arxiv.org/pdf/1811.01459.pdf>https://arxiv.org/pdf/1811.01459.pdf</a></li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNhMr5EXFyUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Losses:\n",
        "  centers = tf.Variable(tf.constant_initializer(0.0)(shape=[751,256],dtype=tf.float32),trainable=False,name='centers')\n",
        "  \n",
        "  @staticmethod\n",
        "  def _pairwise_distances(embeddings, squared=False):\n",
        "      \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
        "\n",
        "      Args:\n",
        "          embeddings: tensor of shape (batch_size, embed_dim)\n",
        "          squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
        "                  If false, output is the pairwise euclidean distance matrix.\n",
        "\n",
        "      Returns:\n",
        "          pairwise_distances: tensor of shape (batch_size, batch_size)\n",
        "      \"\"\"\n",
        "      # Get the dot product between all embeddings\n",
        "      # shape (batch_size, batch_size)\n",
        "      dot_product = tf.matmul(embeddings, tf.transpose(embeddings))\n",
        "\n",
        "      # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
        "      # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
        "      # shape (batch_size,)\n",
        "      square_norm = tf.linalg.diag_part(dot_product)\n",
        "\n",
        "      # Compute the pairwise distance matrix as we have:\n",
        "      # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
        "      # shape (batch_size, batch_size)\n",
        "      distances = tf.expand_dims(square_norm, 0) - 2.0 * dot_product + tf.expand_dims(square_norm, 1)\n",
        "\n",
        "      # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
        "      distances = tf.maximum(distances, 0.0)\n",
        "\n",
        "      if not squared:\n",
        "          # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
        "          # we need to add a small epsilon where distances == 0.0\n",
        "          mask = tf.cast(tf.equal(distances, 0.0),tf.float32)\n",
        "          distances = distances + mask * 1e-16\n",
        "          distances = tf.sqrt(distances)\n",
        "\n",
        "          # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n",
        "          distances = distances * (1.0 - mask)\n",
        "\n",
        "      return distances\n",
        "\n",
        "  @staticmethod\n",
        "  def _masked_maximum(data, mask, dim=1):\n",
        "      axis_minimums = tf.math.reduce_min(data, dim, keepdims=True)\n",
        "      masked_maximums = (\n",
        "          tf.math.reduce_max(\n",
        "              tf.math.multiply(data - axis_minimums, mask), dim, keepdims=True\n",
        "          )\n",
        "          + axis_minimums\n",
        "      )\n",
        "      return masked_maximums\n",
        "\n",
        "  @staticmethod\n",
        "  def _masked_minimum(data, mask, dim=1):\n",
        "      axis_maximums = tf.math.reduce_max(data, dim, keepdims=True)\n",
        "      masked_minimums = (\n",
        "          tf.math.reduce_min(\n",
        "              tf.math.multiply(data - axis_maximums, mask), dim, keepdims=True\n",
        "          )\n",
        "          + axis_maximums\n",
        "      )\n",
        "      return masked_minimums\n",
        "\n",
        "  @staticmethod\n",
        "  def triplet_semihard_loss(\n",
        "      labels,\n",
        "      embeddings,\n",
        "      margin = 1.0,\n",
        "      distance_metric = \"L2\",\n",
        "  ) -> tf.Tensor:\n",
        "      labels = tf.argmax(labels, axis=1)\n",
        "      \n",
        "      precise_embeddings = tf.cast(embeddings, tf.float32)\n",
        "\n",
        "      # Reshape label tensor to [batch_size, 1].\n",
        "      lshape = tf.shape(labels)\n",
        "      labels = tf.reshape(labels, [lshape[0], 1])\n",
        "\n",
        "      # Build pairwise squared distance matrix\n",
        "\n",
        "      if distance_metric == \"L1\":\n",
        "          pdist_matrix = Losses._pairwise_distances(\n",
        "              precise_embeddings, squared=False\n",
        "          )\n",
        "\n",
        "      else:\n",
        "          pdist_matrix = Losses._pairwise_distances(\n",
        "              precise_embeddings, squared=True\n",
        "          )\n",
        "\n",
        "      # Build pairwise binary adjacency matrix.\n",
        "      adjacency = tf.math.equal(labels, tf.transpose(labels))\n",
        "      # Invert so we can select negatives only.\n",
        "      adjacency_not = tf.math.logical_not(adjacency)\n",
        "\n",
        "      batch_size = tf.size(labels)\n",
        "\n",
        "      # Compute the mask.\n",
        "      pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])\n",
        "      mask = tf.math.logical_and(\n",
        "          tf.tile(adjacency_not, [batch_size, 1]),\n",
        "          tf.math.greater(\n",
        "              pdist_matrix_tile, tf.reshape(tf.transpose(pdist_matrix), [-1, 1])\n",
        "          ),\n",
        "      )\n",
        "      mask_final = tf.reshape(\n",
        "          tf.math.greater(\n",
        "              tf.math.reduce_sum(\n",
        "                  tf.cast(mask, dtype=tf.float32), 1, keepdims=True\n",
        "              ),\n",
        "              0.0,\n",
        "          ),\n",
        "          [batch_size, batch_size],\n",
        "      )\n",
        "      mask_final = tf.transpose(mask_final)\n",
        "\n",
        "      adjacency_not = tf.cast(adjacency_not, dtype=tf.float32)\n",
        "      mask = tf.cast(mask, dtype=tf.float32)\n",
        "\n",
        "      # negatives_outside: smallest D_an where D_an > D_ap.\n",
        "      negatives_outside = tf.reshape(\n",
        "          Losses._masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size]\n",
        "      )\n",
        "      negatives_outside = tf.transpose(negatives_outside)\n",
        "\n",
        "      # negatives_inside: largest D_an.\n",
        "      negatives_inside = tf.tile(\n",
        "          Losses._masked_maximum(pdist_matrix, adjacency_not), [1, batch_size]\n",
        "      )\n",
        "      semi_hard_negatives = tf.where(mask_final, negatives_outside, negatives_inside)\n",
        "\n",
        "      loss_mat = tf.math.add(margin, pdist_matrix - semi_hard_negatives)\n",
        "\n",
        "      mask_positives = tf.cast(adjacency, dtype=tf.float32) - tf.linalg.diag(\n",
        "          tf.ones([batch_size])\n",
        "      )\n",
        "\n",
        "      # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
        "      #   in semihard, they take all positive pairs except the diagonal.\n",
        "      num_positives = tf.math.reduce_sum(mask_positives)\n",
        "\n",
        "      triplet_loss = tf.math.truediv(\n",
        "          tf.math.reduce_sum(\n",
        "              tf.math.maximum(tf.math.multiply(loss_mat, mask_positives), 0.0)\n",
        "          ),\n",
        "          num_positives,\n",
        "      )\n",
        "      return tf.cast(triplet_loss, embeddings.dtype)\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def osm_caa_loss(embeddings,labels,weights,alpha=1.2,sigma=0.8,l_parameter=0.5):\n",
        "\n",
        "      '''\n",
        "      alpha : margin of weighted contrastive loss, as mentioned in the paper \n",
        "      l     : hyperparameter controlling weights of positive set and the negative set  \n",
        "      sigma :  sigma OSM (0.8) as mentioned in paper\n",
        "\n",
        "      embeddings : feature vector : (n x d)\n",
        "      labels : (n,)\n",
        "      weights : Fully Connected weights of classification layer (dxC), C is the number of classes: represents the vectors for class\n",
        "      returns online soft mining class attention aware adjusted contrastive loss \n",
        "      '''\n",
        "\n",
        "      labels = tf.argmax(labels, axis=1)\n",
        "      n = labels.shape[0]\n",
        "      n = 32\n",
        "      dist = Losses._pairwise_distances(embeddings)\n",
        "\n",
        "      p_mask = tf.cast(tf.equal(labels[:, tf.newaxis], labels[tf.newaxis, :]), tf.float32)\n",
        "\n",
        "      n_mask = 1- p_mask\n",
        "\n",
        "      S = tf.exp(-dist / (sigma**2 ) )\n",
        "      S_N = tf.clip_by_value(tf.nn.relu(alpha - dist), clip_value_min=tf.constant(1e-12) , clip_value_max=tf.constant(1e12)) \n",
        "      S_P = S * p_mask\n",
        "      S_N = S_N * n_mask\n",
        "      S  = S_P + S_N\n",
        "      weights = tf.math.l2_normalize(weights, axis=0)\n",
        "      denom = tf.reduce_sum(tf.exp(tf.matmul(embeddings , weights)),1)\n",
        "      num =  tf.exp (tf.reduce_sum( embeddings * tf.transpose(tf.gather(weights , labels , axis=1)) , 1 ))\n",
        "\n",
        "      atten_class = num / denom\n",
        "\n",
        "      \n",
        "      temp = tf.tile(tf.expand_dims(atten_class, 0),[n,1])\n",
        "\n",
        "      A =  tf.math.minimum(temp , tf.transpose(temp))\n",
        "\n",
        "      W = S * A\n",
        "      W_P = W * p_mask\n",
        "      W_N = W * n_mask\n",
        "      W_P = W_P * (1 - tf.eye(n))\n",
        "      W_N = W_N * (1 - tf.eye(n))\n",
        "\n",
        "      L_P =  tf.reduce_sum(W_P * tf.pow(dist,2) ) / (2 * tf.reduce_sum(W_P) )\n",
        "      L_N  = tf.reduce_sum(W_N * tf.pow(S_N , 2) ) / (2 * tf.reduce_sum(W_N) )\n",
        "      loss = (1-l_parameter) * L_P + l_parameter * L_N\n",
        "\n",
        "      return loss\n",
        "\n",
        "  @staticmethod\n",
        "  def get_center_loss(features, labels, alpha, num_classes):\n",
        "    labels = tf.argmax(labels, axis=1)\n",
        "    labels = tf.reshape(labels, [-1])\n",
        "    labels = tf.cast(labels,tf.int32)\n",
        "\n",
        "    centers_batch = tf.gather(Losses.centers, labels)\n",
        "\n",
        "    #loss = tf.nn.l2_loss(features - centers_batch)\n",
        "\n",
        "    diff = centers_batch - features\n",
        "    diff = (1 - alpha)*(centers_batch-features)\n",
        "    labels = tf.reshape(labels, [32, 1])\n",
        "    Losses.centers = tf.tensor_scatter_nd_sub(Losses.centers,labels,diff)\n",
        "    loss = tf.reduce_mean(tf.square(features-centers_batch))\n",
        "    # unique_label, unique_idx, unique_count = tf.unique_with_counts(labels)\n",
        "\n",
        "    # appear_times = tf.gather(unique_count, unique_idx)\n",
        "    # appear_times = tf.reshape(appear_times, [-1, 1])\n",
        "\n",
        "    # diff = diff / tf.cast((1 + appear_times), tf.float32)\n",
        "    # loss = tf.reduce_sum(tf.abs(diff))\n",
        "    # diff = alpha * diff\n",
        "\n",
        "    #labels=tf.reshape(labels, [32, 1])\n",
        "    #Losses.centers = tf.tensor_scatter_nd_sub(Losses.centers, labels, diff)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoClTKBi1bDp",
        "colab_type": "text"
      },
      "source": [
        "<h1>Utility functions</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKRAc2kdJDc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encoding(labels):\n",
        "  size = np.max(labels)+1\n",
        "  encode = np.eye(size)[labels]\n",
        "  return encode\n",
        "\n",
        "def get_batches(tr_img,tr_lb):\n",
        "  '''randomly select 4 images each of 8 classes'''\n",
        "  temp = np.arange(0,751)\n",
        "  #randomly select 8 out of 1501 indices\n",
        "  eight_indices = np.random.choice(temp,8,replace=False)\n",
        "  #randomly select 8 labels\n",
        "  labels = tr_lb[eight_indices]\n",
        "  #repeat them 4 times to get 32 labels\n",
        "  labels = np.repeat(labels,4,axis=0)\n",
        "  #initialize train list\n",
        "  train = []\n",
        "  #for each class of image selected according to\n",
        "  #8 random indices, select 4 random images\n",
        "  for i in tr_img[eight_indices]:\n",
        "      x = np.arange(i.shape[0])\n",
        "      try:\n",
        "          train.append(i[np.random.choice(x,4,replace=False)])\n",
        "      except:\n",
        "          train.append(i[np.random.choice(x,4,replace=True)])\n",
        "\n",
        "  #reshape the selected 32 images into 4D tensor\n",
        "  train = np.array(train).reshape(32,128,64,3)\n",
        "\n",
        "  #generate 32 indices\n",
        "  x=np.arange(0,32)\n",
        "  #shuffle the 32 indices\n",
        "  np.random.shuffle(x)\n",
        "  #shuffle the 32 training images\n",
        "  train = train[x]\n",
        "  #shuffle the 32 training labels\n",
        "  labels = labels[x]\n",
        "  return train.astype('float32'),labels.astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2nS7vWmtgVW",
        "colab_type": "text"
      },
      "source": [
        "<h1>Model and Parameters</h1>\n",
        "<b>EfficientNetB0</b> model trained on ImageNet dataset is used as the baseline model. <a href=https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html>Efficient Net link</a><br>\n",
        "A 256 layer dense layer is attached to the output of convolution layers from <b>EfficientNetB0</b> model.\n",
        "\n",
        "<ul>\n",
        "<li>Input Images shape : 128 x 64 x 3</li>\n",
        "<li>Batch size : 32</li>\n",
        "<li>Beta Ratio : 0.3 <br><i>This is used to weigh losses source: <a href=https://arxiv.org/pdf/1912.05295v1.pdf>https://arxiv.org/pdf/1912.05295v1.pdf</a></i></li>\n",
        "<ul>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mmbg0R0ajOLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "BETA_RATIO = 0.3\n",
        "IMG_SIZE = (128, 64)\n",
        "IMG_SHAPE = IMG_SIZE + (3,)\n",
        "file = open('train_dataset.pkl','rb')\n",
        "X_train,Y_train = pickle.load(file)\n",
        "\n",
        "train_images = X_train/255.0\n",
        "train_labels = one_hot_encoding(Y_train)\n",
        "num_classes = train_images.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "base_model = tf.keras.applications.EfficientNetB0(input_shape=IMG_SHAPE,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "\n",
        "\n",
        "\n",
        "inputs = tf.keras.Input(shape=(128, 64, 3))\n",
        "x = base_model(inputs, training=True)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "x = tf.keras.layers.Dense(256)(x)\n",
        "out1 =tf.math.l2_normalize(x,axis=1)\n",
        "x = tf.keras.layers.ReLU()(x)\n",
        "out2 = tf.keras.layers.Dense(num_classes,activation='softmax')(x)\n",
        "model = tf.keras.Model(inputs,[out1,out2])\n",
        "\n",
        "#model.layers[-1].weights[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv0UYxpB40LZ",
        "colab_type": "text"
      },
      "source": [
        "<h1>Combined Loss function</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPrwVV5tlSw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_loss(model,x,y,training):\n",
        "  output =  model(x, training=training)\n",
        "  #cross_entropy_loss\n",
        "  classification_loss = tf.reduce_mean(tf.keras.losses.CategoricalCrossentropy()(y,output[1]))\n",
        "  \n",
        "  #triplet_loss\n",
        "  triplet_loss = Losses.triplet_semihard_loss(labels=y,embeddings=output[0],margin=1.0)\n",
        "  \n",
        "  #osm_caa_loss\n",
        "  osm_caa_loss = Losses.osm_caa_loss(output[0],y,model.layers[-1].weights[0])\n",
        " \n",
        "  #center_loss\n",
        "  center_loss = Losses.get_center_loss(output[0],y,0.5,751)\n",
        "  \n",
        "  total_loss = (1-BETA_RATIO)*triplet_loss + classification_loss + BETA_RATIO*osm_caa_loss + 0.3*center_loss\n",
        "  return total_loss\n",
        "\n",
        "def grad(model, inputs, targets):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss_value = custom_loss(model, inputs, targets, training=True)\n",
        "  return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXut_4mj5EPB",
        "colab_type": "text"
      },
      "source": [
        "<h1>Optimizer</h1>\n",
        "Adam optimzer with the learning rate of 10^(-3) and default parameters is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CamaiJLIMVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDPFfoNl6vbh",
        "colab_type": "text"
      },
      "source": [
        "<h1>Training</h1>\n",
        "The model was trained for 25 epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU-OUiDeFbNN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8a7b8032-f91a-4072-ea2e-064d729f1949"
      },
      "source": [
        "## Note: Rerunning this cell uses the same model variables\n",
        "\n",
        "# Keep results for plotting\n",
        "train_loss_results = []\n",
        "train_accuracy_results = []\n",
        "\n",
        "num_epochs = 25\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "  epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "  \n",
        "  # Training loop - using batches of 32\n",
        "\n",
        "  for i in range(int(16000/BATCH_SIZE)):\n",
        "    x_batch,y_batch = get_batches(train_images,train_labels)\n",
        "    # Optimize the model\n",
        "    loss_value, grads = grad(model, x_batch, y_batch)\n",
        "    if i %100 == 0:\n",
        "      print('epoch : ',epoch, ' i : ',i,' loss : ',loss_value)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Track progress\n",
        "    epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n",
        "    # Compare predicted label to actual label\n",
        "    # training=True is needed only if there are layers with different\n",
        "    # behavior during training versus inference (e.g. Dropout).\n",
        "    epoch_accuracy.update_state(np.argmax(y_batch,axis=1), model(x_batch, training=True)[1])\n",
        "\n",
        "  # End epoch\n",
        "  train_loss_results.append(epoch_loss_avg.result())\n",
        "  train_accuracy_results.append(epoch_accuracy.result())\n",
        "\n",
        "  if epoch % 1 == 0:\n",
        "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
        "                                                                epoch_loss_avg.result(),\n",
        "                                                                epoch_accuracy.result()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch :  0  i :  0  loss :  tf.Tensor(7.9116387, shape=(), dtype=float32)\n",
            "epoch :  0  i :  100  loss :  tf.Tensor(7.7166877, shape=(), dtype=float32)\n",
            "epoch :  0  i :  200  loss :  tf.Tensor(7.3374968, shape=(), dtype=float32)\n",
            "epoch :  0  i :  300  loss :  tf.Tensor(6.8200884, shape=(), dtype=float32)\n",
            "epoch :  0  i :  400  loss :  tf.Tensor(6.5597672, shape=(), dtype=float32)\n",
            "Epoch 000: Loss: 7.156, Accuracy: 6.275%\n",
            "epoch :  1  i :  0  loss :  tf.Tensor(6.8179426, shape=(), dtype=float32)\n",
            "epoch :  1  i :  100  loss :  tf.Tensor(4.524348, shape=(), dtype=float32)\n",
            "epoch :  1  i :  200  loss :  tf.Tensor(5.4620004, shape=(), dtype=float32)\n",
            "epoch :  1  i :  300  loss :  tf.Tensor(4.8847513, shape=(), dtype=float32)\n",
            "epoch :  1  i :  400  loss :  tf.Tensor(5.916371, shape=(), dtype=float32)\n",
            "Epoch 001: Loss: 5.657, Accuracy: 22.094%\n",
            "epoch :  2  i :  0  loss :  tf.Tensor(3.9640744, shape=(), dtype=float32)\n",
            "epoch :  2  i :  100  loss :  tf.Tensor(3.4657428, shape=(), dtype=float32)\n",
            "epoch :  2  i :  200  loss :  tf.Tensor(3.511604, shape=(), dtype=float32)\n",
            "epoch :  2  i :  300  loss :  tf.Tensor(2.955741, shape=(), dtype=float32)\n",
            "epoch :  2  i :  400  loss :  tf.Tensor(3.6434562, shape=(), dtype=float32)\n",
            "Epoch 002: Loss: 3.748, Accuracy: 51.981%\n",
            "epoch :  3  i :  0  loss :  tf.Tensor(2.631602, shape=(), dtype=float32)\n",
            "epoch :  3  i :  100  loss :  tf.Tensor(3.3573222, shape=(), dtype=float32)\n",
            "epoch :  3  i :  200  loss :  tf.Tensor(2.3520572, shape=(), dtype=float32)\n",
            "epoch :  3  i :  300  loss :  tf.Tensor(2.3349571, shape=(), dtype=float32)\n",
            "epoch :  3  i :  400  loss :  tf.Tensor(2.29565, shape=(), dtype=float32)\n",
            "Epoch 003: Loss: 2.429, Accuracy: 75.025%\n",
            "epoch :  4  i :  0  loss :  tf.Tensor(2.1339927, shape=(), dtype=float32)\n",
            "epoch :  4  i :  100  loss :  tf.Tensor(2.749604, shape=(), dtype=float32)\n",
            "epoch :  4  i :  200  loss :  tf.Tensor(2.0135105, shape=(), dtype=float32)\n",
            "epoch :  4  i :  300  loss :  tf.Tensor(1.8020481, shape=(), dtype=float32)\n",
            "epoch :  4  i :  400  loss :  tf.Tensor(2.0361917, shape=(), dtype=float32)\n",
            "Epoch 004: Loss: 1.824, Accuracy: 86.350%\n",
            "epoch :  5  i :  0  loss :  tf.Tensor(1.6497302, shape=(), dtype=float32)\n",
            "epoch :  5  i :  100  loss :  tf.Tensor(1.3047134, shape=(), dtype=float32)\n",
            "epoch :  5  i :  200  loss :  tf.Tensor(1.5062805, shape=(), dtype=float32)\n",
            "epoch :  5  i :  300  loss :  tf.Tensor(1.3906544, shape=(), dtype=float32)\n",
            "epoch :  5  i :  400  loss :  tf.Tensor(1.1365017, shape=(), dtype=float32)\n",
            "Epoch 005: Loss: 1.383, Accuracy: 93.063%\n",
            "epoch :  6  i :  0  loss :  tf.Tensor(1.0882638, shape=(), dtype=float32)\n",
            "epoch :  6  i :  100  loss :  tf.Tensor(1.3609135, shape=(), dtype=float32)\n",
            "epoch :  6  i :  200  loss :  tf.Tensor(0.63965935, shape=(), dtype=float32)\n",
            "epoch :  6  i :  300  loss :  tf.Tensor(1.5160502, shape=(), dtype=float32)\n",
            "epoch :  6  i :  400  loss :  tf.Tensor(0.9672954, shape=(), dtype=float32)\n",
            "Epoch 006: Loss: 1.247, Accuracy: 95.319%\n",
            "epoch :  7  i :  0  loss :  tf.Tensor(1.7142076, shape=(), dtype=float32)\n",
            "epoch :  7  i :  100  loss :  tf.Tensor(1.725123, shape=(), dtype=float32)\n",
            "epoch :  7  i :  200  loss :  tf.Tensor(0.9454769, shape=(), dtype=float32)\n",
            "epoch :  7  i :  300  loss :  tf.Tensor(1.2648554, shape=(), dtype=float32)\n",
            "epoch :  7  i :  400  loss :  tf.Tensor(0.95835036, shape=(), dtype=float32)\n",
            "Epoch 007: Loss: 1.181, Accuracy: 96.194%\n",
            "epoch :  8  i :  0  loss :  tf.Tensor(0.9335816, shape=(), dtype=float32)\n",
            "epoch :  8  i :  100  loss :  tf.Tensor(2.103072, shape=(), dtype=float32)\n",
            "epoch :  8  i :  200  loss :  tf.Tensor(0.60214335, shape=(), dtype=float32)\n",
            "epoch :  8  i :  300  loss :  tf.Tensor(0.90995353, shape=(), dtype=float32)\n",
            "epoch :  8  i :  400  loss :  tf.Tensor(0.5851654, shape=(), dtype=float32)\n",
            "Epoch 008: Loss: 1.074, Accuracy: 97.419%\n",
            "epoch :  9  i :  0  loss :  tf.Tensor(0.97564423, shape=(), dtype=float32)\n",
            "epoch :  9  i :  100  loss :  tf.Tensor(1.2211881, shape=(), dtype=float32)\n",
            "epoch :  9  i :  200  loss :  tf.Tensor(0.73988676, shape=(), dtype=float32)\n",
            "epoch :  9  i :  300  loss :  tf.Tensor(1.7694722, shape=(), dtype=float32)\n",
            "epoch :  9  i :  400  loss :  tf.Tensor(0.9803288, shape=(), dtype=float32)\n",
            "Epoch 009: Loss: 1.070, Accuracy: 97.869%\n",
            "epoch :  10  i :  0  loss :  tf.Tensor(0.6144806, shape=(), dtype=float32)\n",
            "epoch :  10  i :  100  loss :  tf.Tensor(0.9399618, shape=(), dtype=float32)\n",
            "epoch :  10  i :  200  loss :  tf.Tensor(1.0769024, shape=(), dtype=float32)\n",
            "epoch :  10  i :  300  loss :  tf.Tensor(1.1140476, shape=(), dtype=float32)\n",
            "epoch :  10  i :  400  loss :  tf.Tensor(0.5924959, shape=(), dtype=float32)\n",
            "Epoch 010: Loss: 1.062, Accuracy: 97.869%\n",
            "epoch :  11  i :  0  loss :  tf.Tensor(0.95169026, shape=(), dtype=float32)\n",
            "epoch :  11  i :  100  loss :  tf.Tensor(0.929006, shape=(), dtype=float32)\n",
            "epoch :  11  i :  200  loss :  tf.Tensor(1.0546389, shape=(), dtype=float32)\n",
            "epoch :  11  i :  300  loss :  tf.Tensor(1.0874349, shape=(), dtype=float32)\n",
            "epoch :  11  i :  400  loss :  tf.Tensor(1.269387, shape=(), dtype=float32)\n",
            "Epoch 011: Loss: 0.984, Accuracy: 98.281%\n",
            "epoch :  12  i :  0  loss :  tf.Tensor(1.0982523, shape=(), dtype=float32)\n",
            "epoch :  12  i :  100  loss :  tf.Tensor(1.4778681, shape=(), dtype=float32)\n",
            "epoch :  12  i :  200  loss :  tf.Tensor(0.740362, shape=(), dtype=float32)\n",
            "epoch :  12  i :  300  loss :  tf.Tensor(0.6470425, shape=(), dtype=float32)\n",
            "epoch :  12  i :  400  loss :  tf.Tensor(1.2399801, shape=(), dtype=float32)\n",
            "Epoch 012: Loss: 1.014, Accuracy: 98.300%\n",
            "epoch :  13  i :  0  loss :  tf.Tensor(0.60595024, shape=(), dtype=float32)\n",
            "epoch :  13  i :  100  loss :  tf.Tensor(0.6143288, shape=(), dtype=float32)\n",
            "epoch :  13  i :  200  loss :  tf.Tensor(0.53061175, shape=(), dtype=float32)\n",
            "epoch :  13  i :  300  loss :  tf.Tensor(0.9834477, shape=(), dtype=float32)\n",
            "epoch :  13  i :  400  loss :  tf.Tensor(0.69834614, shape=(), dtype=float32)\n",
            "Epoch 013: Loss: 0.937, Accuracy: 98.581%\n",
            "epoch :  14  i :  0  loss :  tf.Tensor(0.77389324, shape=(), dtype=float32)\n",
            "epoch :  14  i :  100  loss :  tf.Tensor(0.64508516, shape=(), dtype=float32)\n",
            "epoch :  14  i :  200  loss :  tf.Tensor(0.53034794, shape=(), dtype=float32)\n",
            "epoch :  14  i :  300  loss :  tf.Tensor(1.0767722, shape=(), dtype=float32)\n",
            "epoch :  14  i :  400  loss :  tf.Tensor(1.09078, shape=(), dtype=float32)\n",
            "Epoch 014: Loss: 0.895, Accuracy: 98.838%\n",
            "epoch :  15  i :  0  loss :  tf.Tensor(1.8478465, shape=(), dtype=float32)\n",
            "epoch :  15  i :  100  loss :  tf.Tensor(1.1867179, shape=(), dtype=float32)\n",
            "epoch :  15  i :  200  loss :  tf.Tensor(0.5027626, shape=(), dtype=float32)\n",
            "epoch :  15  i :  300  loss :  tf.Tensor(1.5391712, shape=(), dtype=float32)\n",
            "epoch :  15  i :  400  loss :  tf.Tensor(0.66547, shape=(), dtype=float32)\n",
            "Epoch 015: Loss: 0.960, Accuracy: 98.456%\n",
            "epoch :  16  i :  0  loss :  tf.Tensor(0.7881953, shape=(), dtype=float32)\n",
            "epoch :  16  i :  100  loss :  tf.Tensor(0.76882285, shape=(), dtype=float32)\n",
            "epoch :  16  i :  200  loss :  tf.Tensor(0.4423151, shape=(), dtype=float32)\n",
            "epoch :  16  i :  300  loss :  tf.Tensor(0.5128521, shape=(), dtype=float32)\n",
            "epoch :  16  i :  400  loss :  tf.Tensor(0.58506095, shape=(), dtype=float32)\n",
            "Epoch 016: Loss: 0.883, Accuracy: 98.875%\n",
            "epoch :  17  i :  0  loss :  tf.Tensor(1.5292863, shape=(), dtype=float32)\n",
            "epoch :  17  i :  100  loss :  tf.Tensor(0.6279139, shape=(), dtype=float32)\n",
            "epoch :  17  i :  200  loss :  tf.Tensor(0.67219883, shape=(), dtype=float32)\n",
            "epoch :  17  i :  300  loss :  tf.Tensor(0.9360045, shape=(), dtype=float32)\n",
            "epoch :  17  i :  400  loss :  tf.Tensor(0.7522527, shape=(), dtype=float32)\n",
            "Epoch 017: Loss: 0.912, Accuracy: 98.869%\n",
            "epoch :  18  i :  0  loss :  tf.Tensor(0.69433224, shape=(), dtype=float32)\n",
            "epoch :  18  i :  100  loss :  tf.Tensor(1.6224623, shape=(), dtype=float32)\n",
            "epoch :  18  i :  200  loss :  tf.Tensor(0.47416922, shape=(), dtype=float32)\n",
            "epoch :  18  i :  300  loss :  tf.Tensor(1.040181, shape=(), dtype=float32)\n",
            "epoch :  18  i :  400  loss :  tf.Tensor(0.9876759, shape=(), dtype=float32)\n",
            "Epoch 018: Loss: 0.898, Accuracy: 98.956%\n",
            "epoch :  19  i :  0  loss :  tf.Tensor(1.0940524, shape=(), dtype=float32)\n",
            "epoch :  19  i :  100  loss :  tf.Tensor(0.6861393, shape=(), dtype=float32)\n",
            "epoch :  19  i :  200  loss :  tf.Tensor(0.899445, shape=(), dtype=float32)\n",
            "epoch :  19  i :  300  loss :  tf.Tensor(0.8052373, shape=(), dtype=float32)\n",
            "epoch :  19  i :  400  loss :  tf.Tensor(1.0034956, shape=(), dtype=float32)\n",
            "Epoch 019: Loss: 0.862, Accuracy: 98.937%\n",
            "epoch :  20  i :  0  loss :  tf.Tensor(1.1090397, shape=(), dtype=float32)\n",
            "epoch :  20  i :  100  loss :  tf.Tensor(1.2798227, shape=(), dtype=float32)\n",
            "epoch :  20  i :  200  loss :  tf.Tensor(0.8345563, shape=(), dtype=float32)\n",
            "epoch :  20  i :  300  loss :  tf.Tensor(0.47542018, shape=(), dtype=float32)\n",
            "epoch :  20  i :  400  loss :  tf.Tensor(0.6123487, shape=(), dtype=float32)\n",
            "Epoch 020: Loss: 0.893, Accuracy: 99.013%\n",
            "epoch :  21  i :  0  loss :  tf.Tensor(1.0585322, shape=(), dtype=float32)\n",
            "epoch :  21  i :  100  loss :  tf.Tensor(1.2809712, shape=(), dtype=float32)\n",
            "epoch :  21  i :  200  loss :  tf.Tensor(0.7112308, shape=(), dtype=float32)\n",
            "epoch :  21  i :  300  loss :  tf.Tensor(1.6092988, shape=(), dtype=float32)\n",
            "epoch :  21  i :  400  loss :  tf.Tensor(0.84298944, shape=(), dtype=float32)\n",
            "Epoch 021: Loss: 0.874, Accuracy: 98.925%\n",
            "epoch :  22  i :  0  loss :  tf.Tensor(0.6690089, shape=(), dtype=float32)\n",
            "epoch :  22  i :  100  loss :  tf.Tensor(0.54582894, shape=(), dtype=float32)\n",
            "epoch :  22  i :  200  loss :  tf.Tensor(1.3636643, shape=(), dtype=float32)\n",
            "epoch :  22  i :  300  loss :  tf.Tensor(0.4307816, shape=(), dtype=float32)\n",
            "epoch :  22  i :  400  loss :  tf.Tensor(0.45642456, shape=(), dtype=float32)\n",
            "Epoch 022: Loss: 0.846, Accuracy: 99.044%\n",
            "epoch :  23  i :  0  loss :  tf.Tensor(0.94292307, shape=(), dtype=float32)\n",
            "epoch :  23  i :  100  loss :  tf.Tensor(0.53266585, shape=(), dtype=float32)\n",
            "epoch :  23  i :  200  loss :  tf.Tensor(0.38361615, shape=(), dtype=float32)\n",
            "epoch :  23  i :  300  loss :  tf.Tensor(1.3729212, shape=(), dtype=float32)\n",
            "epoch :  23  i :  400  loss :  tf.Tensor(0.60707974, shape=(), dtype=float32)\n",
            "Epoch 023: Loss: 0.885, Accuracy: 98.969%\n",
            "epoch :  24  i :  0  loss :  tf.Tensor(1.0751419, shape=(), dtype=float32)\n",
            "epoch :  24  i :  100  loss :  tf.Tensor(0.95115703, shape=(), dtype=float32)\n",
            "epoch :  24  i :  200  loss :  tf.Tensor(0.47090632, shape=(), dtype=float32)\n",
            "epoch :  24  i :  300  loss :  tf.Tensor(1.4436904, shape=(), dtype=float32)\n",
            "epoch :  24  i :  400  loss :  tf.Tensor(0.6967878, shape=(), dtype=float32)\n",
            "Epoch 024: Loss: 0.870, Accuracy: 98.887%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQbmwVhJtr3z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "af58e927-6cee-4bf4-ad72-76de6aa03dd7"
      },
      "source": [
        "!mkdir -p saved_model\n",
        "model.save('saved_model/cl_model.h5')\n",
        "!zip -r /content/file.zip /content/saved_model/\n",
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/saved_model/ (stored 0%)\n",
            "  adding: content/saved_model/cl_model.h5 (deflated 8%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6f43f0f4-bc49-450d-95b0-a0eb092eda88\", \"file.zip\", 25519829)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAg8U1epm5vD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4303113d-d2fd-4b65-a4dd-90fa0cfba5b1"
      },
      "source": [
        "x_batch,y_batch = get_batches(train_images,train_labels)\n",
        "np.argmax(y_batch,axis=1)==np.argmax(model(x_batch,training=False)[1],axis=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlJm5xXFNlM8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7946685d-6cec-4291-e777-ccaa52bb0917"
      },
      "source": [
        "#Testing code(outputs rank 1 accuracy)\n",
        "BATCH_SIZE = 200\n",
        "\n",
        "def find_l2_norm(a,b):\n",
        "    '''\n",
        "    a : 2D numpy array\n",
        "    b : 2D numpy array\n",
        "    returns the L2_norm between each vector of a and each vector of b\n",
        "    if a : 4 x 256 and b : 7 x 256\n",
        "       output is 4 x 7 norm matrix\n",
        "\n",
        "     '''\n",
        "    try:\n",
        "        a = a.reshape([1,256])\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        b = b.reshape([1,256])\n",
        "    except:\n",
        "        pass\n",
        "    dot_product = np.matmul(a, np.transpose(b))\n",
        "    a = np.square(a)\n",
        "    b = np.square(b)\n",
        "    norm = np.sum((np.expand_dims(a,axis=1) + b), axis=2) - 2*dot_product\n",
        "    norm = np.sqrt(norm)\n",
        "    return norm\n",
        "\n",
        "def stats(correct,incorrect):\n",
        "    correct = np.around(correct,2)\n",
        "    incorrect = np.around(incorrect,2)\n",
        "    mean_correct,mean_incorrect = np.mean(correct),np.mean(incorrect)\n",
        "    md_correct,md_incorrect = np.median(correct),np.median(incorrect)\n",
        "    sd_correct,sd_incorrect = np.std(correct),np.std(incorrect)\n",
        "    min_correct,max_correct = np.min(correct),np.max(correct)\n",
        "    min_incorrect,max_incorrect = np.min(incorrect),np.max(incorrect)\n",
        "\n",
        "    print('\\n\\n')\n",
        "    print('Correct distance')\n",
        "    print('mean_correct : ',mean_correct)\n",
        "    print('sd_correct : ',sd_correct)\n",
        "    print('md_correct : ',md_correct)\n",
        "    print('min : ',min_correct, '  ','max : ',max_correct)\n",
        "    print('val : ', (np.sum(np.cast['int'](correct<0.33)) / np.sum(np.cast['int'](correct>0.0)))*100 )\n",
        "\n",
        "    print('\\n\\n')\n",
        "    print('Incorrect distance')\n",
        "    print('mean_incorrect : ',mean_incorrect)\n",
        "    print('sd_incorrect : ',sd_incorrect)\n",
        "    print('md_incorrect : ',md_incorrect)\n",
        "    print('min : ',min_incorrect, '  ','max : ',max_incorrect)\n",
        "    print('\\n\\n')\n",
        "\n",
        "\n",
        "\n",
        "def market_test():\n",
        "\n",
        "    '''\n",
        "    output : ouputs the rank-1 accuracy\n",
        "    \n",
        "    '''\n",
        "    file=open('test_dataset.pkl','rb')\n",
        "    test_mat,test_imgs,query_mat,query_imgs = pickle.load(file)\n",
        "    test_embed=get_embeddings(test_mat)\n",
        "    query_embed= get_embeddings(query_mat)\n",
        "    test_embed = np.array(test_embed)\n",
        "    query_embed = np.array(query_embed)\n",
        "    print('no problems with embeddings')\n",
        "    print(test_mat.shape)\n",
        "    print(query_mat.shape)\n",
        "    #counts the number of correct query images identified\n",
        "    correct  = 0\n",
        "    #counts the total number of query images\n",
        "    tot = 0 \n",
        "    #keeps track of query embeddings to be tested\n",
        "    counter = 0\n",
        "    #number of embeddings which are tested at once\n",
        "    batch_size = 100\n",
        "\n",
        "    correct_dist=[]\n",
        "    incorrect_dist=[]\n",
        "\n",
        "    tot_num = query_embed.shape[0]\n",
        "\n",
        "    while(counter+batch_size < tot_num):\n",
        "        distances = find_l2_norm(query_embed[counter:counter+batch_size],test_embed)\n",
        "        imgs = query_imgs[counter:counter+batch_size]\n",
        "        for i in range(distances.shape[0]):\n",
        "            dis = distances[i].reshape([-1])\n",
        "            count = zip(dis,test_imgs)\n",
        "            count2 = sorted(count,key=lambda x:x[0])\n",
        "            try:\n",
        "              if int(imgs[i].split('_')[0]) == int(count2[0][1].split('_')[0]):\n",
        "                  correct += 1\n",
        "                  correct_dist.append(count2[0][0])\n",
        "            except:\n",
        "              print('hello')\n",
        "            else:\n",
        "                incorrect_dist.append(count2[0][0])\n",
        "            tot += 1\n",
        "        counter += batch_size\n",
        "        print(correct)\n",
        "    \n",
        "    distances = find_l2_norm(query_embed[counter:tot_num],test_embed)\n",
        "    imgs = query_imgs[counter:tot_num]\n",
        "    for i in range(distances.shape[0]):\n",
        "        dis = distances[i].reshape([-1])\n",
        "        count = zip(dis,test_imgs)\n",
        "        count2 = sorted(count,key=lambda x:x[0])\n",
        "        if int(imgs[i].split('_')[0]) == int(count2[0][1].split('_')[0]):\n",
        "            correct += 1\n",
        "            correct_dist.append(count2[0][0])\n",
        "        else:\n",
        "            incorrect_dist.append(count2[0][0])\n",
        "        tot += 1\n",
        "    counter += batch_size\n",
        "\n",
        "    stats(correct_dist,incorrect_dist)\n",
        "\n",
        "    print('rank 1 accuracy : ',(correct/tot)*100,'%')\n",
        "\n",
        "\n",
        "def get_embeddings(imgs):\n",
        "  '''\n",
        "  imgs : numpy array of list of images of shape [n X 128 x 64 x 3]\n",
        "  outputs : outputs the embeddings after passing through the \n",
        "            base model\n",
        "  '''\n",
        "  embeddings = []\n",
        "  count = 0\n",
        "  tot_size = imgs.shape[0]\n",
        "  print(tot_size)\n",
        "\n",
        "  while (count + BATCH_SIZE ) < tot_size :\n",
        "    x = imgs[count:count+BATCH_SIZE]\n",
        "    output = model.predict(x)[0]\n",
        "    embeddings+=list(output)\n",
        "    count += BATCH_SIZE\n",
        "\n",
        "  x = imgs[count:tot_size]\n",
        "  \n",
        "  output = model.predict(x)[0]\n",
        "  embeddings+=list(output)\n",
        "  print(len(embeddings))\n",
        "  return embeddings\n",
        "\n",
        "\n",
        "market_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19732\n",
            "19732\n",
            "3368\n",
            "3368\n",
            "no problems with embeddings\n",
            "(19732, 128, 64, 3)\n",
            "(3368, 128, 64, 3)\n",
            "96\n",
            "188\n",
            "279\n",
            "369\n",
            "466\n",
            "561\n",
            "655\n",
            "751\n",
            "845\n",
            "937\n",
            "1034\n",
            "1128\n",
            "1226\n",
            "1319\n",
            "1413\n",
            "1507\n",
            "1602\n",
            "1698\n",
            "1792\n",
            "1887\n",
            "1984\n",
            "2080\n",
            "2169\n",
            "2268\n",
            "2364\n",
            "2458\n",
            "2553\n",
            "2648\n",
            "2743\n",
            "2840\n",
            "2936\n",
            "3033\n",
            "3124\n",
            "\n",
            "\n",
            "\n",
            "Correct distance\n",
            "mean_correct :  0.47260353\n",
            "sd_correct :  0.121952936\n",
            "md_correct :  0.46\n",
            "min :  0.2    max :  0.9\n",
            "val :  9.284818067754077\n",
            "\n",
            "\n",
            "\n",
            "Incorrect distance\n",
            "mean_incorrect :  0.48527846\n",
            "sd_incorrect :  0.13193332\n",
            "md_incorrect :  0.47\n",
            "min :  0.2    max :  0.9\n",
            "\n",
            "\n",
            "\n",
            "rank 1 accuracy :  94.65558194774347 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUkujK3ciXB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p saved_model\n",
        "!mkdir -p saved_model\n",
        "model.save('saved_model/my_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLLYTnKqfj3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r /content/file.zip /content/saved_model/\n",
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMsn65LFgktj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('my_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbnLUh_pRE4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdgHvhl5vqwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "model = tf.keras.models.load_model('my_model.h5')\n",
        "files = os.listdir('.')\n",
        "imgs = []\n",
        "labels = []\n",
        "for img in files:\n",
        "  try:\n",
        "    if img.split('.')[1]=='jpg':\n",
        "      mat = cv2.imread(img)\n",
        "      imgs.append(mat)\n",
        "      labels.append(int(img.split('_')[0]))\n",
        "  except:\n",
        "    print('ok')\n",
        "imgs=np.array(imgs)\n",
        "imgs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTRow92cQ6fg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = model.predict(imgs)\n",
        "features[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loTMrrSVRSiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_l2_norm(a,b):\n",
        "    '''\n",
        "    a : 2D numpy array\n",
        "    b : 2D numpy array\n",
        "    returns the L2_norm between each vector of a and each vector of b\n",
        "    if a : 4 x 256 and b : 7 x 256\n",
        "       output is 4 x 7 norm matrix\n",
        "\n",
        "     '''\n",
        "    try:\n",
        "        a = a.reshape([1,256])\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        b = b.reshape([1,256])\n",
        "    except:\n",
        "        pass\n",
        "    dot_product = np.matmul(a, np.transpose(b))\n",
        "    a = np.square(a)\n",
        "    b = np.square(b)\n",
        "    norm = np.sum((np.expand_dims(a,axis=1) + b), axis=2) - 2*dot_product + 1e-6\n",
        "    norm = np.sqrt(norm)\n",
        "    return norm\n",
        "\n",
        "out = find_l2_norm(features[0],features[0])\n",
        "out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyujDMwhSg0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfGwlAcTSlfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}